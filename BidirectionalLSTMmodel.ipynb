{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-output": false
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import concatenate\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df=pd.read_csv('../input/train.csv')\ndftest=pd.read_csv('../input/test.csv')\nprint(df.shape, dftest.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc811e824e6bbb9b15f14daadabafe75d0e13633"
      },
      "cell_type": "code",
      "source": "train, val = train_test_split(df, test_size=0.1, random_state=1995)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2c368ac6cb84910eb8110784c87818c70f55fb4"
      },
      "cell_type": "code",
      "source": "train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac3305801f5d53536fe38f78c13662e46685dd43"
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nstop = stopwords.words('english')\ntrain['question_text'] = train['question_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ntrain['question_text'][0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5bc1ca7bf9ccaf43d42da7f70bdd45a6038e9046"
      },
      "cell_type": "code",
      "source": "train['question_text'] = train['question_text'].str.replace('[^\\w\\s]','')\ntrain['question_text'][0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0eb6e8a0859bc4994162d841e3d441100c1ea411"
      },
      "cell_type": "code",
      "source": "train_X = train[\"question_text\"].fillna(\"_na_\").values\nval_X = val[\"question_text\"].fillna(\"_na_\").values\ntest_X = dftest[\"question_text\"].fillna(\"_na_\").values\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3839130bf3eab7141ad6e00a69f9a26279d542db"
      },
      "cell_type": "code",
      "source": "embed_size = 300 # how big is each word vector\nmax_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 70 # max number of words in a question to use",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7fa157c40f99129312ffbaab8bf8e77c10b53e4b"
      },
      "cell_type": "code",
      "source": "tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c024fabc2c9ac9678655dbf6febbc8b69a4270a"
      },
      "cell_type": "code",
      "source": "train_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3985af0ec3ed1fd2d6287ae7bab9409192deddc7"
      },
      "cell_type": "code",
      "source": "train_y = train_df['target'].values\nval_y = val_df['target'].values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7554a9312c0dd392e48abd5bec82eaaca6d210fd"
      },
      "cell_type": "code",
      "source": "train_X.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a7d38d97ca9bff262256ee9e740b5fd6d8d842a9"
      },
      "cell_type": "code",
      "source": "def attention_3d_block(inputs):\n    # inputs.shape = (batch_size, time_steps, input_dim)\n    TIME_STEPS = inputs.shape[1].value\n    SINGLE_ATTENTION_VECTOR = False\n    \n    input_dim = int(inputs.shape[2])\n    a = Permute((2, 1))(inputs)\n    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n    a = Dense(TIME_STEPS, activation='softmax')(a)\n    if SINGLE_ATTENTION_VECTOR:\n        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n        a = RepeatVector(input_dim)(a)\n    a_probs = Permute((2, 1))(a)\n    output_attention_mul = Multiply()([inputs, a_probs])\n    return output_attention_mul",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a33fa2ba3f8e26edf6e115b3757e9af7e0a657dd"
      },
      "cell_type": "code",
      "source": "from keras import backend as K\nfrom keras.engine.topology import Layer, InputSpec\nfrom keras import initializers\n\nclass AttLayer(Layer):\n    def __init__(self, attention_dim):\n        self.init = initializers.get('normal')\n        self.supports_masking = True\n        self.attention_dim = attention_dim\n        super(AttLayer, self).__init__()\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n        self.b = K.variable(self.init((self.attention_dim, )))\n        self.u = K.variable(self.init((self.attention_dim, 1)))\n        self.trainable_weights = [self.W, self.b, self.u]\n        super(AttLayer, self).build(input_shape)\n\n    def compute_mask(self, inputs, mask=None):\n        return mask\n\n    def call(self, x, mask=None):\n        # size of x :[batch_size, sel_len, attention_dim]\n        # size of u :[batch_size, attention_dim]\n        # uit = tanh(xW+b)\n        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n        ait = K.dot(uit, self.u)\n        ait = K.squeeze(ait, -1)\n\n        ait = K.exp(ait)\n\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            ait *= K.cast(mask, K.floatx())\n        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        ait = K.expand_dims(ait)\n        weighted_input = x * ait\n        output = K.sum(weighted_input, axis=1)\n\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[-1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2fd499ce052e73a5f3ef90fb09a2b8088728086b"
      },
      "cell_type": "code",
      "source": "import gc",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "13469dfd022efeab994817f4a19559a3a21fde77"
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_1 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix_1[i] = embedding_vector\n\ndel embeddings_index; gc.collect() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc84213e4a0481827a44071c7884e9ba36763df6"
      },
      "cell_type": "code",
      "source": "MODEL2 = model2()\nMODEL2.summary()\n\nbatch_size = 2048\nepochs = 5\n\nearly_stopping = EarlyStopping(patience=3, verbose=1, monitor='val_loss', mode='min')\nmodel_checkpoint = ModelCheckpoint('./model2.model', save_best_only=True, verbose=1, monitor='val_loss', mode='min')\nreduce_lr = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n\nhist = MODEL2.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, validation_data=(val_X, val_y), verbose=True)\nMODEL2.save('./model2.h5')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a5f91e75ae835dbf0a216f478841215555d8091"
      },
      "cell_type": "code",
      "source": "pred_val_y_2 = MODEL2.predict([val_X], batch_size=1024, verbose=1)\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = metrics.f1_score(val_y, (pred_val_y_2 > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh_2 = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh_2)\n\ny_pred_2 = MODEL2.predict(test_X, batch_size=1024, verbose=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ab60c15c0f6483b376994319649397561c49d01"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n%matplotlib inline\nOrder = [1, 2, 3, 4]\nF1scores_epochs = [0.6327, 0.6607, 0.6688, 0.6698]\n\nLABELS = [\"1\", \"2\",\"3\", \"5\"]\nplt.figure(figsize = (10,5))\nplt.bar(Order, F1scores_epochs, align='center', width=0.3)\nplt.xticks(Order, LABELS)\nplt.xlabel('Epochs')\nplt.ylabel('F-1 scores')\nplt.ylim(0.63, 0.675)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ed7f9eacb491016c84b715f8835202758e21d07"
      },
      "cell_type": "code",
      "source": "Order = [1, 2, 3, 4]\nVal_Accuracy = [0.9546, 0.9572, 0.9578, 0.9578]\n\nLABELS = [\"1\", \"2\",\"3\", \"5\"]\nplt.figure(figsize = (10,5))\nplt.bar(Order, Val_Accuracy, align='center', width=0.3)\nplt.xticks(Order, LABELS)\nplt.xlabel('Epochs')\nplt.ylabel('Validation Accuracy')\nplt.ylim(0.94, 0.96)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b078b653f27722ec51a635685964de597e029f73"
      },
      "cell_type": "code",
      "source": "y_te = (y_pred_2[:,0] > best_thresh_2).astype(np.int)\n\nsubmit_df = pd.DataFrame({\"qid\": dftest[\"qid\"], \"prediction\": y_te})\nsubmit_df.to_csv(\"submission.csv\", index=False)\nsub = pd.read_csv('../input/sample_submission.csv')\nsub.to_csv(\"submission.csv\", index=False)",
      "execution_count": 98,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e3659914e53e35d51812bc70090088380c84eabb"
      },
      "cell_type": "code",
      "source": "#from IPython.display import HTML\n#import base64  \n#import pandas as pd  \n\n#def create_download_link( df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    #csv = df.to_csv(index =False)\n    #b64 = base64.b64encode(csv.encode())\n    #payload = b64.decode()\n    #html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    #html = html.format(payload=payload,title=title,filename=filename)\n    #return HTML(html)\n\n#create_download_link(submit_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-output": false,
        "trusted": true,
        "_uuid": "1e0c2a0d67ae8963ab893bdf490fcf001280e33f"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "585335c52850254e6182f3116e5e914286a8f10b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}